{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47c116d2",
   "metadata": {},
   "source": [
    "Messages =>\n",
    "\n",
    "\n",
    "Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM. Messages are objects that contain:\n",
    "Role - Identifies the message type (e.g. system, user)\n",
    "Content - Represents the actual content of the message (like text, images, audio, documents, etc.)\n",
    "Metadata - Optional fields such as response information, message IDs, and token usage\n",
    "LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb699e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GEMINI_API_KEY']=os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model=init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n",
    "response=model.invoke(\"Write me a contrasting paragraph on langchain\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd1e89",
   "metadata": {},
   "source": [
    "Text Prompts->\n",
    "Strings ideal forr the straight forward generation task where we \n",
    "don't need to retain history\n",
    "\"Write me a contrasting paragraph on langchain\" ==> Is a text prompt \n",
    "Use Text Prompts when :=>\n",
    "1. Single,standalone request\n",
    "2. Minimal Code Complexity\n",
    "3. Don't need conversation history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd55357",
   "metadata": {},
   "source": [
    "Message Prompts->\n",
    "\n",
    "1. System Message -> It tells model how to behave and provide context for interaction\n",
    "    \n",
    "    e.g -> Act as a data Scientist and teach me this\n",
    "\n",
    "2. Human Message ->  Represent user input and interaction with model\n",
    "\n",
    "3. AI Message -> What LLM model generate\n",
    "4. Tool Message -> Represent output of the tool calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e771f6e",
   "metadata": {},
   "source": [
    "System Message ->\n",
    "\n",
    "A SystemMessage represent an initial set of instructions that primes the model’s behavior. You can use a system message to set the tone, define the model’s role, and establish guidelines for responses.\n",
    "\n",
    "Human Message->\n",
    "\n",
    "A HumanMessage represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal content.\n",
    "\n",
    "AI Message->\n",
    "\n",
    "An AIMessage represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.\n",
    "\n",
    "Tool Message->\n",
    "\n",
    "For models that support tool calling, AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example-> 1\n",
    "from langchain.messages import SystemMessage,ToolMessage,AIMessage,HumanMessage\n",
    "messages=[\n",
    "    SystemMessage(\"You are a dataScience Expert\"),\n",
    "    HumanMessage(\"Explain me a machine learning algorithm\"),\n",
    "]\n",
    "response=model.invoke(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example-> 2\n",
    "system_msg=SystemMessage(\"You are a helpful coding assistant\")\n",
    "messages=[\n",
    "    system_msg,\n",
    "    HumanMessage(\"How do I create a REST API ?\")\n",
    "]\n",
    "response=model.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7411d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Detailed INfo To LLM with System Message\n",
    "from langchain.messages import SystemMessage,HumanMessage\n",
    "sys_msg=SystemMessage(\n",
    "\"\"\"\n",
    "You are a python Senior Developer with experience in web frameworks.\n",
    "Always provide code examples and explain your reasoning.\n",
    "Be concide but thorough in explanation\n",
    "\"\"\")\n",
    "messages=[\n",
    "    sys_msg,\n",
    "    HumanMessage(\"How do I create a restful API\")\n",
    "]\n",
    "response=model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Message Meta Data\n",
    "human_msg=HumanMessage(\n",
    "    content=\"Hello!\",\n",
    "    name=\"alice\", # Optional: Identify User \n",
    "    id=\"msg_123\", # optional: Unique identifier for tracking\n",
    ")\n",
    "# response=model.invoke(human_msg)\n",
    "response=model.invoke([human_msg])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c919e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import AIMessage, SystemMessage, HumanMessage\n",
    "#Create an Al message manually (e.g., for conversation history)\n",
    "ai_msg= AIMessage(\"I'd be happy to help you with that question!\")\n",
    "\n",
    "#Add to conversation history\n",
    "messages=[\n",
    "SystemMessage(\"You are a helpful assistant\"),\n",
    "HumanMessage(\"Can you help me?\"),\n",
    "ai_msg, # Insert as if it came from the model\n",
    "HumanMessage(\"Great! What's 2+2?\")\n",
    "]\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76be49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6e03103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import AIMessage\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "# After a model make a request here how create the message for brevity\n",
    "ai_msg=AIMessage(\n",
    "    content=[],\n",
    "    tool_calls=[{\n",
    "        'name':\"get_weather\",\n",
    "        \"args\":{\"location\":\"New York\"},\n",
    "        \"id\":\"call_132\",\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1777ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute tool and create result message\n",
    "weather_result=\"Sunny, 72F\"\n",
    "tool_message=ToolMessage(\n",
    "    content=weather_result,\n",
    "    tool_call_id=\"call_132\" # Must match the call Id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "adeeea22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The weather in New York is **sunny** and the temperature is **72°F**.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019bd076-96f5-7ca2-89bf-adde4e587191-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 46, 'output_tokens': 19, 'total_tokens': 65, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages=[\n",
    "    HumanMessage(content=\"What is the weather like in New York?\"),\n",
    "    ai_msg,\n",
    "    tool_message,\n",
    "]\n",
    "response=model.invoke(messages)\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
